{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "import argparse\n",
    "import pandas as pd\n",
    "import sqlalchemy\n",
    "import numpy as np\n",
    "from numpy import mean\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "from scipy.stats import randint as sp_randint\n",
    "from scipy.stats import uniform as sp_uniform\n",
    "\n",
    "\n",
    "import xgboost as xgb \n",
    "import lightgbm as lgb\n",
    "from sklearn.externals import joblib\n",
    "from sklearn.metrics import classification_report, confusion_matrix, f1_score, accuracy_score\n",
    "from sklearn.model_selection import cross_val_score, RepeatedStratifiedKFold, RandomizedSearchCV\n",
    "from sklearn.linear_model import LogisticRegression, SGDClassifier, LogisticRegression\n",
    "from sklearn.tree import DecisionTreeClassifier, DecisionTreeRegressor\n",
    "from sklearn.ensemble import BaggingClassifier, RandomForestClassifier,\\\n",
    "                             AdaBoostClassifier, GradientBoostingClassifier,\\\n",
    "                             GradientBoostingRegressor, RandomForestRegressor, BaggingRegressor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "def postgres_connector(host, port, database, user, password=None):\n",
    "    user_info = user if password is None else user + ':' + password\n",
    "    url = 'postgres://%s@%s:%d/%s' % (user_info, host, port, database)\n",
    "    return sqlalchemy.create_engine(url, client_encoding='utf-8')\n",
    "\n",
    "def read_data(table):\n",
    "    query='select * from {}'.format(table)\n",
    "    df=pd.read_sql(query, engine)\n",
    "    print('{} table successfully loaded!'.format(table))\n",
    "    return df\n",
    "\n",
    "def little_merge(post_table, t2, table_name):\n",
    "    \n",
    "    #merge tables\n",
    "    df_merge=post_table.merge(t2, on='post_key',how='left')\n",
    "    \n",
    "    #fill missing value\n",
    "    df_merge['created_at_hour_y']=df_merge['created_at_hour_y'].fillna(df_merge['created_at_hour_x'])\n",
    "    df_merge['count']=df_merge['count'].fillna(0)\n",
    "    \n",
    "    #exclude data which is not in training domain(10 hr)\n",
    "    df_merge=df_merge[(df_merge['created_at_hour_y']-\\\n",
    "                       df_merge['created_at_hour_x']).dt.total_seconds()/3600<10]\n",
    "    df_merge=df_merge[(df_merge['created_at_hour_y']-\\\n",
    "                      df_merge['created_at_hour_x']).dt.total_seconds()/3600>=0]\n",
    "    df_merge['time_after_create']=(df_merge['created_at_hour_y']\\\n",
    "                                  -df_merge['created_at_hour_x']).dt.total_seconds()/3600\n",
    "    \n",
    "    #drop non-training columns\n",
    "    df=df_merge.drop(['created_at_hour_x','created_at_hour_y', 'like_count_36_hour'], axis=1)\n",
    "    \n",
    "    #features engineering:generate new training features\n",
    "    df_train=pd.pivot_table(df,values='count',index='post_key',columns='time_after_create')\n",
    "    df_train=df_train.fillna(0)\n",
    "    \n",
    "    #rename the columns\n",
    "    for i in range(10):\n",
    "        df_train=df_train.rename(columns={df_train.columns[i]:'{} hour after {}'.format(i,table_name)})\n",
    "    \n",
    "    #convert float to int\n",
    "    #df_train[list(df_train.columns)]=df_train[list(df_train.columns)].astype(int)\n",
    "    print('posts table and {} table successfully merge!'.format(table_name))\n",
    "    return df_train\n",
    "\n",
    "def post_table_feature_engineer(table):\n",
    "    \n",
    "    #adding new training categorical features: created_weekday, created_hour\n",
    "    table['created_weekday']=table['created_at_hour'].apply(lambda x: x.weekday())\n",
    "    table['created_time']=table['created_at_hour'].apply(lambda x: x.hour)\n",
    "    \n",
    "    #setting is_trending label where 'like_count_36_hour'>1000 => 1 and 'like_count_36_hour'<=1000 =>0\n",
    "    table['is_trending']=table['like_count_36_hour'].apply(lambda x: 1 if x>1000 else 0)\n",
    "    \n",
    "    #drop non_training features\n",
    "    table=table.drop(['created_at_hour','like_count_36_hour'], axis=1)\n",
    "    \n",
    "    print('posts table finish feature engineering!')\n",
    "    return table\n",
    "\n",
    "def big_merge(t1, t2, t3, t4, post_table):\n",
    "    \n",
    "    #outer join the 4 small tables\n",
    "    df1=t1.merge(t2, on='post_key',how='outer')\n",
    "    df2=df1.merge(t3, on='post_key',how='outer')\n",
    "    df3=df2.merge(t4, on='post_key',how='outer')\n",
    "    \n",
    "    #left join post_table\n",
    "    df=post_table.merge(df3, on='post_key',how='left')\n",
    "    \n",
    "    #fill missing values\n",
    "    df=df.fillna(0)\n",
    "    \n",
    "    #convert float to int\n",
    "    df[list(df.columns[1:])]=df[list(df.columns[1:])].astype(int)\n",
    "    print('all table merged successfully!')\n",
    "    return df\n",
    "\n",
    "def create_dummy(data, dummy_features):\n",
    "    df=pd.get_dummies(data,columns=dummy_features, drop_first=True)\n",
    "    print('dummy variable created successfully!')\n",
    "    return df\n",
    "\n",
    "def train_test(train, test):\n",
    "    x_train=train.iloc[:,2:]\n",
    "    y_train=train['is_trending']\n",
    "    x_test=test.iloc[:,2:]\n",
    "    y_test=test['is_trending']\n",
    "    print('training and testing data successfully generate!')\n",
    "    return x_train, y_train, x_test, y_test\n",
    "\n",
    "def lgb_hypertuned_parameters_model(x, y):\n",
    "    # Initiate the Light-GBM Classifier\n",
    "    LGB_clf = lgb.LGBMClassifier(objective='binary',n_estimators=150,max_depth=-1 ,random_state=0)\n",
    "\n",
    "    # Construct ranges for each parameter \n",
    "    param_grid ={'num_leaves': sp_randint(6, 50), \n",
    "                 'min_child_samples': sp_randint(100, 500), \n",
    "                 'min_child_weight': [1e-5, 1e-3, 1e-2, 1e-1, 1, 1e1, 1e2, 1e3, 1e4],\n",
    "                 'subsample': sp_uniform(loc=0.2, scale=0.8), \n",
    "                 'colsample_bytree': sp_uniform(loc=0.4, scale=0.6),\n",
    "                 'reg_alpha': [0, 1e-1, 1, 2, 5, 7, 10, 50, 100],\n",
    "                 'reg_lambda': [0, 1e-1, 1, 5, 10, 20, 50, 100]}\n",
    "\n",
    "    # Do the randomized grid searching 30 times \n",
    "    # Find the optimal combination of parameters such that the highest f1 score attained\n",
    "    grid_obj = RandomizedSearchCV(LGB_clf, param_distributions=param_grid, cv=5, scoring='f1', n_iter = 30)\n",
    "    grid_obj.fit(x, y)\n",
    "    print('model finish training!')\n",
    "    return grid_obj\n",
    "\n",
    "def save_model(model):\n",
    "    joblib.dump(grid_obj, args.save_filepath)\n",
    "    print('model successfully saved!')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if __name__ == '__main__':\n",
    "    \n",
    "    parser = argparse.ArgumentParser(description='Train a Light-gbm classifier for Dcard posts')\n",
    "    parser.add_argument('host', help='input database host')\n",
    "    parser.add_argument('save_filepath', help='filepath where model saved')\n",
    "    args = parser.parse_args()\n",
    "    \n",
    "    \n",
    "    #access database\n",
    "    engine = postgres_connector(\n",
    "    args.host,\n",
    "    5432,\n",
    "    \"intern_task\",\n",
    "    \"candidate\",\n",
    "    \"dcard-data-intern-2020\"\n",
    "    )\n",
    "    \n",
    "    #load training data\n",
    "    #load train tables\n",
    "    df_posts=read_data('posts_train')\n",
    "    df_share=read_data('post_shared_train')\n",
    "    df_comment=read_data('post_comment_created_train')\n",
    "    df_like=read_data('post_liked_train')\n",
    "    df_collect=read_data('post_collected_train')\n",
    "    \n",
    "    #join feature tables and feature engineering\n",
    "    posts_share=little_merge(df_posts,df_share, 'share')\n",
    "    posts_comment=little_merge(df_posts,df_comment, 'comment')\n",
    "    posts_like=little_merge(df_posts,df_like, 'like')\n",
    "    posts_collect=little_merge(df_posts,df_collect, 'collect')\n",
    "\n",
    "    #target table feature engineering\n",
    "    df_posts_train=post_table_feature_engineer(df_posts)\n",
    "\n",
    "    #merge all tables\n",
    "    df=big_merge(posts_share, posts_comment, posts_like, posts_collect, df_posts_train)\n",
    "\n",
    "    #deal with categorical features\n",
    "    df_train=create_dummy(df, ['created_weekday', 'created_time'])\n",
    "    \n",
    "    \n",
    "    #load testing data\n",
    "    #load test tables\n",
    "    df_posts_test=read_data('posts_test')\n",
    "    df_share_test=read_data('post_shared_test')\n",
    "    df_comment_test=read_data('post_comment_created_test')\n",
    "    df_like_test=read_data('post_liked_test')\n",
    "    df_collect_test=read_data('post_collected_test')\n",
    "\n",
    "    #join feature tables and feature engineering\n",
    "    posts_share_test=little_merge(df_posts_test,df_share_test, 'share')\n",
    "    posts_comment_test=little_merge(df_posts_test,df_comment_test, 'comment')\n",
    "    posts_like_test=little_merge(df_posts_test,df_like_test, 'like')\n",
    "    posts_collect_test=little_merge(df_posts_test,df_collect_test, 'collect')\n",
    "\n",
    "    #target table feature engineering\n",
    "    df_posts_test=post_table_feature_engineer(df_posts_test)\n",
    "\n",
    "    #merge all tables\n",
    "    df_t=big_merge(posts_share_test, posts_comment_test, posts_like_test, posts_collect_test, df_posts_test)\n",
    "\n",
    "    #deal with categorical features\n",
    "    df_test=create_dummy(df_t, ['created_weekday', 'created_time'])\n",
    "    \n",
    "    \n",
    "    \n",
    "    #generate train and test data\n",
    "    x_train, y_train, x_test, y_test=train_test(df_train, df_test)\n",
    "    \n",
    "    #fit model\n",
    "    grid_obj=lgb_hypertuned_parameters_model(x_train, y_train)\n",
    "    \n",
    "    #save model\n",
    "    save_model(grid_obj)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
