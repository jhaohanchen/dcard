{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import sqlalchemy\n",
    "import joblib\n",
    "import argparse"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "def postgres_connector(host, port, database, user, password=None):\n",
    "    user_info = user if password is None else user + ':' + password\n",
    "    url = 'postgres://%s@%s:%d/%s' % (user_info, host, port, database)\n",
    "    return sqlalchemy.create_engine(url, client_encoding='utf-8')\n",
    "\n",
    "def read_data(table):\n",
    "    query='select * from {}'.format(table)\n",
    "    df=pd.read_sql(query, engine)\n",
    "    print('{} table successfully loaded!'.format(table))\n",
    "    return df\n",
    "\n",
    "def little_merge(post_table, t2, table_name):\n",
    "    \n",
    "    #merge tables\n",
    "    df_merge=post_table.merge(t2, on='post_key',how='left')\n",
    "    \n",
    "    #fill missing value\n",
    "    df_merge['created_at_hour_y']=df_merge['created_at_hour_y'].fillna(df_merge['created_at_hour_x'])\n",
    "    df_merge['count']=df_merge['count'].fillna(0)\n",
    "    \n",
    "    #exclude data which is not in training domain(10 hr)\n",
    "    df_merge=df_merge[(df_merge['created_at_hour_y']-\\\n",
    "                       df_merge['created_at_hour_x']).dt.total_seconds()/3600<10]\n",
    "    df_merge=df_merge[(df_merge['created_at_hour_y']-\\\n",
    "                      df_merge['created_at_hour_x']).dt.total_seconds()/3600>=0]\n",
    "    df_merge['time_after_create']=(df_merge['created_at_hour_y']\\\n",
    "                                  -df_merge['created_at_hour_x']).dt.total_seconds()/3600\n",
    "    \n",
    "    #drop non-training columns\n",
    "    df=df_merge.drop(['created_at_hour_x','created_at_hour_y'], axis=1)\n",
    "    \n",
    "    #features engineering:generate new training features\n",
    "    df_train=pd.pivot_table(df,values='count',index='post_key',columns='time_after_create')\n",
    "    df_train=df_train.fillna(0)\n",
    "    \n",
    "    #rename the columns\n",
    "    for i in range(10):\n",
    "        df_train=df_train.rename(columns={df_train.columns[i]:'{} hour after {}'.format(i,table_name)})\n",
    "    \n",
    "    #convert float to int\n",
    "    #df_train[list(df_train.columns)]=df_train[list(df_train.columns)].astype(int)\n",
    "    \n",
    "    print('posts_test table and {} table successfully merge!'.format(table_name))\n",
    "    return df_train\n",
    "\n",
    "def post_table_feature_engineer(table):\n",
    "    \n",
    "    #adding new training categorical features: created_weekday, created_hour\n",
    "    table['created_weekday']=table['created_at_hour'].apply(lambda x: x.weekday())\n",
    "    table['created_time']=table['created_at_hour'].apply(lambda x: x.hour)\n",
    "    \n",
    "    #drop non_training features\n",
    "    table=table.drop(['created_at_hour'], axis=1)\n",
    "    \n",
    "    print('posts_test finish feature engineering!')\n",
    "    return table\n",
    "\n",
    "def big_merge(t1, t2, t3, t4, post_table):\n",
    "    \n",
    "    #outer join the 4 small tables\n",
    "    df1=t1.merge(t2, on='post_key',how='outer')\n",
    "    df2=df1.merge(t3, on='post_key',how='outer')\n",
    "    df3=df2.merge(t4, on='post_key',how='outer')\n",
    "    \n",
    "    #left join post_table\n",
    "    df=post_table.merge(df3, on='post_key',how='left')\n",
    "    \n",
    "    #fill missing values\n",
    "    df=df.fillna(0)\n",
    "    \n",
    "    #convert float to int\n",
    "    df[list(df.columns[1:])]=df[list(df.columns[1:])].astype(int)\n",
    "    \n",
    "    # drop non-predicted column\n",
    "    df=df.drop(['post_key'], axis=1)\n",
    "    \n",
    "    print('all table merged successfully!')\n",
    "    return df\n",
    "\n",
    "def create_dummy(data, dummy_features):\n",
    "    \n",
    "    df=pd.get_dummies(data,columns=dummy_features, drop_first=True)\n",
    "\n",
    "    print('dummy variable created successfully!')\n",
    "    return df\n",
    "\n",
    "def load_model(filepath):\n",
    "    \n",
    "    model=joblib.load(filepath)\n",
    "    \n",
    "    print('model successfully loaded!')\n",
    "    return model\n",
    "\n",
    "def predict_model(model, test_data):\n",
    "    \n",
    "    predict=model.predict(test_data)\n",
    "    print('every post is predicted!')\n",
    "    return predict\n",
    "\n",
    "def save_output(data, predict, output_filepath):\n",
    "    \n",
    "    post_key=data.post_key\n",
    "    results = pd.DataFrame({'post_key':post_key, 'is_trending':predict})\n",
    "    results.to_csv(output_filepath,index=False)\n",
    "    \n",
    "    print('output successfully saved!')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if __name__ == '__main__':\n",
    "    \n",
    "    parser = argparse.ArgumentParser(description='Predict if post will become trending post for Dcard posts')\n",
    "    parser.add_argument('host', help='input database host')\n",
    "    parser.add_argument('model', help='filepath where model saved')\n",
    "    parser.add_argument('output', help='filepath where output save')\n",
    "    args = parser.parse_args()\n",
    "    \n",
    "    #access database\n",
    "    engine = postgres_connector(\n",
    "    args.host,\n",
    "    5432,\n",
    "    \"intern_task\",\n",
    "    \"candidate\",\n",
    "    \"dcard-data-intern-2020\"\n",
    "    )\n",
    "    \n",
    "    #load test table\n",
    "    df_posts=read_data('posts_test')\n",
    "    df_share=read_data('post_shared_test')\n",
    "    df_comment=read_data('post_comment_created_test')\n",
    "    df_like=read_data('post_liked_test')\n",
    "    df_collect=read_data('post_collected_test')\n",
    "\n",
    "    #join feature tables and feature engineering\n",
    "    posts_share=little_merge(df_posts,df_share, 'share')\n",
    "    posts_comment=little_merge(df_posts,df_comment, 'comment')\n",
    "    posts_like=little_merge(df_posts,df_like, 'like')\n",
    "    posts_collect=little_merge(df_posts,df_collect, 'collect')\n",
    "\n",
    "    #target table feature engineering\n",
    "    df_posts_test=post_table_feature_engineer(df_posts)\n",
    "\n",
    "    #merge all tables\n",
    "    df=big_merge(posts_share, posts_comment, posts_like, posts_collect, df_posts_test)\n",
    "\n",
    "    #deal with categorical features\n",
    "    df_test=create_dummy(df, ['created_weekday', 'created_time'])\n",
    "    \n",
    "    #load trained model\n",
    "    model=load_model(args.model)\n",
    "    \n",
    "    #predict post\n",
    "    predict=predict_model(model, df_test)\n",
    "    \n",
    "    #save output\n",
    "    save_output(df_posts_test, predict, args.output)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
